{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     1
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Matvecs and matmuls, memory hierarchy, Strassen algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lectures\n",
    "\n",
    "- Floating point arithmetics and related issues\n",
    "- Stable algorithms: backward and forward stability\n",
    "- Most important matrix norms: spectral and Frobenius\n",
    "- Unitary matrices preserve these norms\n",
    "- There are two \"basic\" classes of unitary matrices: Householder and Givens matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of peak performance\n",
    "\n",
    "**Flops** –– floating point operations per second.\n",
    "\n",
    "Giga = $2^{30} \\approx 10^9$,  \n",
    "Tera = $2^{40} \\approx 10^{12}$,      \n",
    "Peta = $2^{50} \\approx 10^{15}$,       \n",
    "Exa = $2^{60} \\approx 10^{18}$ \n",
    "\n",
    "What is the **peak perfomance** of:\n",
    "\n",
    "1. Modern CPU\n",
    "2. Modern GPU\n",
    "3. Largest supercomputer of the world? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clock frequency of CPU vs. performance in flops\n",
    "\n",
    "FLOPS = sockets * (cores per socket) * (number of clock cycles per second) * (number of floating point operations per cycle).\n",
    "\n",
    "- Typically sockets = 1\n",
    "- Number of cores is typically 2 or 4\n",
    "- Number of ticks per second is familiar clock frequency\n",
    "- Number of floating point operations per tick depends on the particular CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "1. Modern CPU (Intel Core i7)   –– 400 Gflops\n",
    "2. Modern GPU [Nvidia DGX H100](https://www.nvidia.com/en-us/data-center/h100/) -- depends on the precision!\n",
    "3. Largest supercomputer in the world –– 1.102 Exaflop/s –– peak performanse \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix-by-vector multiplication (matvec)\n",
    "\n",
    "Multiplication of an $n\\times n$ matrix $A$ by a vector $x$ of size $n\\times 1$ ($y=Ax$):\n",
    "\n",
    "$$\n",
    "y_{i} = \\sum_{j=1}^n a_{ij} x_j\n",
    "$$\n",
    "\n",
    "requires $n^2$ mutliplications and $n(n-1)$ additions. Thus, the overall complexity is $2n^2 - n =$ <font color='red'> $\\mathcal{O}(n^2)$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How bad is $\\mathcal{O}(n^2)$?\n",
    "\n",
    "- Let $A$ be the matrix of pairwise gravitational interaction between planets in a galaxy.\n",
    "\n",
    "- The number of planets in an average galaxy is $10^{11}$, so the size of this matrix is $10^{11} \\times 10^{11}$.\n",
    "\n",
    "- To model evolution in time we have to multiply this matrix by vector at each time step.\n",
    "\n",
    "- Top supercomputers do around $10^{16}$ floating point operations per second (flops), so the time required to multiply the matrix $A$ by a vector is approximately\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{(10^{11})^2 \\text{ operations}}{10^{16} \\text{ flops}} = 10^6 \\text{ sec} \\approx 11.5 \\text{ days} \n",
    "\\end{align*}\n",
    "\n",
    "for one time step. If we could multiply it with $\\mathcal{O}(n)$ complexity, we would get\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{10^{11} \\text{ operations}}{10^{16} \\text{ flops}} = 10^{-5} \\text{ sec}.\n",
    "\\end{align*}\n",
    "\n",
    "Here is the YouTube video that illustrates collision of two galaxisies which was modelled by $\\mathcal{O}(n \\log n)$ algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/7HF5Oy8IMoM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10737e170>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"7HF5Oy8IMoM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we beat $\\mathcal{O}(n^2)$?\n",
    "\n",
    "- Generally speaking **NO**. \n",
    "- The point is that we have $\\mathcal{O}(n^2)$ input data, so there is no way to be faster for a general matrix.\n",
    "- Fortunately, we can be faster <font color='red'>for certain types of matrices</font>.\n",
    "Here are some examples:\n",
    "    * The simplest example may be a matrix of all ones, which can be easily multiplied with only $n-1$ additions. This matrix is of rank one. More generally we can multiply fast by <font color='red'>low-rank </font> matrices (or by matrices that have low-rank blocks)\n",
    "    * <font color='red'>Sparse</font> matrices (contain $\\mathcal{O}(n)$ nonzero elements)\n",
    "    * <font color='red'>Structured</font> matrices:\n",
    "      * Fourier\n",
    "      * Circulant\n",
    "      * Toeplitz \n",
    "      * Hankel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix-by-matrix product\n",
    "\n",
    "Consider composition of two linear operators:\n",
    "\n",
    "1. $y = Bx$\n",
    "2. $z = Ay$\n",
    "\n",
    "Then, $z = Ay =  A B x = C x$, where $C$ is the **matrix-by-matrix product**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix-by-matrix product (MM): classics\n",
    "\n",
    "**Definition**. A product of an $n \\times k$ matrix $A$ and a $k \\times m$ matrix $B$ is a $n \\times m$ matrix $C$ with the elements  \n",
    "$$\n",
    "   c_{ij} = \\sum_{s=1}^k a_{is} b_{sj}, \\quad i = 1, \\ldots, n, \\quad j = 1, \\ldots, m \n",
    "$$\n",
    "\n",
    "For $m=k=n$ complexity of a na&iuml;ve algorithm is $2n^3 - n^2 =$ <font color='red'>$\\mathcal{O}(n^3)$</font>.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion of MM\n",
    "\n",
    "- Matrix-by-matrix product is the **core** for almost all efficient algorithms in numerical linear algebra.  \n",
    "\n",
    "- Basically, all the dense NLA algorithms are reduced to a sequence of matrix-by-matrix products.\n",
    "\n",
    "- Efficient implementation of MM reduces the complexity of numerical algorithms by the same factor.  \n",
    "\n",
    "- However, implementing MM is not easy at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Efficient implementation for MM\n",
    "**Q1**: Is it easy to multiply a matrix by a matrix in the most efficient way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Answer: no, it is not easy\n",
    "\n",
    "If you want it as fast as possible, using the computers that are at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo\n",
    "Let us do a short demo and compare a `np.dot()` procedure which in my case uses MKL with a hand-written matrix-by-matrix routine in Python and also its numba version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def matmul(a, b):\n",
    "    n = a.shape[0]\n",
    "    k = a.shape[1]\n",
    "    m = b.shape[1]  \n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for s in range(k):\n",
    "                c[i, j] += a[i, s] * b[s, j]\n",
    "                \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit # Just-in-time compiler for Python, see http://numba.pydata.org \n",
    "\n",
    "@jit(nopython=True)\n",
    "def numba_matmul(a, b):\n",
    "    n = a.shape[0]\n",
    "    k = a.shape[1]\n",
    "    m = b.shape[1]\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            for s in range(k):\n",
    "                c[i, j] += a[i, s] * b[s, j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we just compare computational times.\n",
    "\n",
    "Guess the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251 ms ± 3.52 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "541 μs ± 4.84 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "10.4 μs ± 15.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from jax.config import config\n",
    "#config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "n = 100\n",
    "a = np.random.randn(n, n)\n",
    "b = np.random.randn(n, n)\n",
    "\n",
    "a_jax = np.array(a)\n",
    "b_jax = np.array(b)\n",
    "\n",
    "%timeit matmul(a, b)\n",
    "%timeit numba_matmul(a, b)\n",
    "%timeit a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is this answer correct for any dimensions of matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension = 10\n",
      "Dimension = 20\n",
      "Dimension = 30\n",
      "Dimension = 40\n",
      "Dimension = 50\n",
      "Dimension = 60\n",
      "Dimension = 70\n",
      "Dimension = 80\n",
      "Dimension = 90\n",
      "Dimension = 100\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dim_range = [10*i for i in range(1, 11)]\n",
    "time_range_matmul = []\n",
    "time_range_numba_matmul = []\n",
    "time_range_np = []\n",
    "for n in dim_range:\n",
    "    print(\"Dimension = {}\".format(n))\n",
    "    a = np.random.randn(n, n)\n",
    "    b = np.random.randn(n, n)\n",
    "\n",
    "    t = %timeit -o -q matmul(a, b)\n",
    "    time_range_matmul.append(t.best)\n",
    "    t = %timeit -o -q numba_matmul(a, b)\n",
    "    time_range_numba_matmul.append(t.best)\n",
    "    t = %timeit -o -q np.dot(a, b)\n",
    "    time_range_np.append(t.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dim_range, time_range_matmul, label=\"Matmul\")\n",
    "plt.plot(dim_range, time_range_numba_matmul, label=\"Matmul Numba\")\n",
    "plt.plot(dim_range, time_range_np, label=\"Numpy\")\n",
    "plt.legend(fontsize=18)\n",
    "plt.xlabel(\"Dimension\", fontsize=18)\n",
    "plt.ylabel(\"Time\", fontsize=18)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is na&iuml;ve implementation slow?\n",
    "It is slow due to two issues:\n",
    "\n",
    "- It does not use the benefits of fast memory (cache) and in general memory architecture\n",
    "- It does not use available parallelization ability (especially important for GPU) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory architecture\n",
    "<img width=80% src=\"Memory-Hierarchy.jpg\">\n",
    "\n",
    "- Fast memory is small\n",
    "- Bigger memory is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Typical Memory Hierarchy Specifications\n",
    "\n",
    " | Memory Type | Size | Access Time | Notes |\n",
    " |------------|------|-------------|--------|\n",
    " | CPU Registers | Few KB | <1 ns | Fastest, directly accessed by CPU |\n",
    " | L1 Cache | 32-64 KB | 1-4 ns | Split into instruction and data cache |\n",
    " | L2 Cache | 256 KB - 1 MB | 4-10 ns | Unified cache |\n",
    " | L3 Cache | 2-32 MB | 10-20 ns | Shared between CPU cores |\n",
    " | Main Memory (RAM) | 8-32 GB | 100 ns | Primary system memory |\n",
    " | SSD | 256 GB - 2 TB | 10-100 μs | Fast secondary storage |\n",
    " | Hard Drive | 1-10 TB | 5-10 ms | Slowest but largest storage |\n",
    "\n",
    " Key observations:\n",
    " - Access time increases ~10x at each level\n",
    " - Size increases ~10-100x at each level\n",
    " - Effective use of faster memory levels is crucial for performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cache Lines and Cache Coherence\n",
    " \n",
    " - Cache memory is organized into **cache lines** - fixed-size blocks (typically 64 bytes)\n",
    " - When CPU needs data, it loads entire cache line containing that data\n",
    " - This is efficient when accessing sequential memory (spatial locality)\n",
    " \n",
    " **Cache coherence** ensures that:\n",
    " - Multiple CPU cores see consistent view of memory\n",
    " - When one core modifies data, other cores are notified\n",
    " - Prevents race conditions and data inconsistency\n",
    " \n",
    " Why it matters for matrix operations:\n",
    " - Sequential access to matrix rows/columns affects cache line utilization\n",
    " - Poor cache line usage = more cache misses = slower performance\n",
    " - Multi-threaded code needs coherent caches for correctness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Memory Architecture\n",
    " \n",
    " Modern GPUs have a different memory hierarchy compared to CPUs:\n",
    " \n",
    " | Memory Type | Size | Access Time | Notes |\n",
    " |------------|------|-------------|--------|\n",
    " | Registers | ~4 MB per SM | ~1 clock | Fastest, per thread/block |\n",
    " | Shared Memory/L1 Cache | 64-256 KB per SM | ~20-30 clocks | Shared within thread block |\n",
    " | L2 Cache | 512KB - 60MB | ~200 clocks | Shared across GPU |\n",
    " | Global Memory (VRAM) | 16-80 GB | ~400-600 clocks | Main GPU memory |\n",
    " | System RAM | 8-128 GB | >1000 clocks | CPU memory, accessed via PCIe |\n",
    " \n",
    " Key differences from CPU:\n",
    " - Much more parallel access (thousands of threads)\n",
    " - Larger register file but smaller caches\n",
    " - Higher memory bandwidth but higher latency\n",
    " - Coalesced memory access critical for performance\n",
    " \n",
    " Memory access patterns:\n",
    " - Coalesced: threads in a warp access consecutive memory = fast\n",
    " - Strided/random: threads access scattered memory = slow\n",
    " - Shared memory bank conflicts can limit bandwidth\n",
    " \n",
    " Best practices:\n",
    " - Use shared memory for frequently accessed data\n",
    " - Ensure coalesced global memory access\n",
    " - Minimize data transfer between CPU and GPU\n",
    " \n",
    " Note: The latest NVIDIA H100 GPU can have up to 80GB of HBM3 VRAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Making algorithms more computationally intensive \n",
    "\n",
    "<font color='red'>**Implementation in NLA**</font>: use block version of algorithms. <br>\n",
    "\n",
    "This approach is a core of **[BLAS (Basic Linear Algebra Subroutines)](http://www.netlib.org/blas/)**, written in Fortran many years ago, and still rules the computational world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Split the matrix into blocks! For illustration consider splitting in $2 \\times 2$ block matrix:\n",
    "\n",
    "$$\n",
    "   A = \\begin{bmatrix}\n",
    "         A_{11} & A_{12} \\\\\n",
    "         A_{21} & A_{22}\n",
    "        \\end{bmatrix}, \\quad B = \\begin{bmatrix}\n",
    "         B_{11} & B_{12} \\\\\n",
    "         B_{21} & B_{22}\n",
    "        \\end{bmatrix}$$\n",
    "\n",
    "Then,  \n",
    "\n",
    "$$AB = \\begin{bmatrix}A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\\\\n",
    "            A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\end{bmatrix}.$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If $A_{11}, B_{11}$ and their product fit into the cache memory (which is 20 Mb (L3) for the [recent Intel Chip](https://en.wikipedia.org/wiki/List_of_Intel_processors#Desktop_(codenamed_%22Comet_Lake%22))), then we load them only once into the memory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BLAS\n",
    "BLAS has three levels:\n",
    "1. BLAS-1, operations like $c = a + b$\n",
    "2. BLAS-2, operations like matrix-by-vector product\n",
    "3. BLAS-3, matrix-by-matrix product\n",
    "\n",
    "What is the principal differences between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main difference is the number of operations vs. the number of input data!\n",
    "\n",
    "1. BLAS-1: $\\mathcal{O}(n)$ data, $\\mathcal{O}(n)$ operations\n",
    "2. BLAS-2: $\\mathcal{O}(n^2)$ data, $\\mathcal{O}(n^2)$ operations\n",
    "3. BLAS-3: $\\mathcal{O}(n^2)$ data, $\\mathcal{O}(n^3)$ operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why BLAS is so important and actual?\n",
    "\n",
    "1. The state-of-the-art implementation of the basic linear algebra operations\n",
    "2. Provides standard names for operations in any new implementations (e.g. [ATLAS](https://www.netlib.org/atlas/), [OpenBLAS](https://www.openblas.net/), [MKL](https://software.intel.com/en-us/mkl)). You can call matrix-by-matrix multiplication function (GEMM), link your code with any BLAS implementation and it will work correctly\n",
    "3. Formulate new algorithms in terms of BLAS operations\n",
    "4. There are wrappers for the most popular languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Packages related to BLAS\n",
    " \n",
    "1. [ATLAS](http://math-atlas.sourceforge.net) - Automatic Tuned Linear Algebra Software. It automatically adapts to a particular system architechture. \n",
    "2. [LAPACK](http://www.netlib.org/lapack/) - Linear Algebra Package. It provides high-level linear algebra operations (e.g. matrix factorizations), which are based on calls of BLAS subroutines.\n",
    "3. [Intel MKL](https://software.intel.com/en-us/intel-mkl) - Math Kernel Library. It provides re-implementation of BLAS and LAPACK, optimized for Intel processors. Available in Anaconda Python distribution: \n",
    " ```\n",
    " conda install mkl\n",
    " ```\n",
    " \n",
    "4. OpenBLAS is an optimized BLAS library based on [GotoBLAS](https://en.wikipedia.org/wiki/GotoBLAS). \n",
    " \n",
    "5. PyTorch [supports](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations) some calls from BLAS and LAPACK\n",
    "6. For NVIDIA GPUs, [cuBLAS](https://docs.nvidia.com/cuda/cublas/index.html) provides a GPU-accelerated implementation of BLAS.\n",
    " \n",
    "7. For AMD GPUs, [rocBLAS](https://rocmdocs.amd.com/en/latest/ROCm_Libraries/ROCm_Libraries.html#rocblas) is part of the ROCm platform and offers a BLAS implementation optimized for AMD hardware.\n",
    " \n",
    "For comparison of OpenBLAS and Intel MKL, see this [review](https://software.intel.com/en-us/articles/performance-comparison-of-openblas-and-intel-math-kernel-library-in-r)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Faster algorithms for matrix multiplication\n",
    "\n",
    "Recall that matrix-matrix multiplication costs $\\mathcal{O}(n^3)$ operations.\n",
    "However, storage is $\\mathcal{O}(n^2)$.\n",
    "\n",
    "**Question:** is it possible to reduce number operations down to $\\mathcal{O}(n^2)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer**: a quest for $\\mathcal{O}(n^2)$ matrix-by-matrix multiplication algorithm is not yet done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Strassen gives $\\mathcal{O}(n^{2.807\\dots})$ –– sometimes used in practice \n",
    "\n",
    "* [Current world record](http://arxiv.org/pdf/1401.7714v1.pdf) $\\mathcal{O}(n^{2.37\\dots})$ –– big constant, not practical, based on [Coppersmith-Winograd_algorithm](https://en.wikipedia.org/wiki/Coppersmith%E2%80%93Winograd_algorithm).\n",
    "- It improved the previous record (Williams 2012) by $3\\cdot 10^{-7}$\n",
    "- The papers still study multiplication of $3 \\times 3$ matrices and interpret it from different sides ([Heule, et. al. 2019](https://arxiv.org/pdf/1905.10192.pdf)) \n",
    "\n",
    "Consider Strassen in more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Na&iuml;ve multiplication\n",
    "\n",
    "Let $A$ and $B$ be two $2\\times 2$ matrices. Na&iuml;ve multiplication $C = AB$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} c_{11} & c_{12} \\\\ c_{21} & c_{22}  \\end{bmatrix}  =\n",
    "\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}  \\end{bmatrix}\n",
    "\\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22}  \\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{21} + a_{12}b_{22} \\\\ \n",
    "a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{21} + a_{22}b_{22} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "contains $8$ multiplications and $4$ additions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strassen algorithm\n",
    "\n",
    "In the work [Gaussian elimination is not optimal](http://link.springer.com/article/10.1007%2FBF02165411?LI=true) (1969) Strassen found that one can calculate $C$ using 18 additions and only 7 multiplications:\n",
    "$$\n",
    "\\begin{split}\n",
    "c_{11} &= f_1 + f_4 - f_5 + f_7, \\\\\n",
    "c_{12} &= f_3 + f_5, \\\\\n",
    "c_{21} &= f_2 + f_4, \\\\\n",
    "c_{22} &= f_1 - f_2 + f_3 + f_6,\n",
    "\\end{split}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{split}\n",
    "f_1 &= (a_{11} + a_{22}) (b_{11} + b_{22}), \\\\\n",
    "f_2 &= (a_{21} + a_{22}) b_{11}, \\\\\n",
    "f_3 &= a_{11} (b_{12} - b_{22}), \\\\\n",
    "f_4 &= a_{22} (b_{21} - b_{11}), \\\\\n",
    "f_5 &= (a_{11} + a_{12}) b_{22}, \\\\\n",
    "f_6 &= (a_{21} - a_{11}) (b_{11} + b_{12}), \\\\\n",
    "f_7 &= (a_{12} - a_{22}) (b_{21} + b_{22}).\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Fortunately, these formulas hold even if $a_{ij}$ and $b_{ij}$, $i,j=1,2$ are block matrices.\n",
    "\n",
    "Thus, Strassen algorithm looks as follows. \n",
    "- First of all we <font color='red'>split</font> matrices $A$ and $B$ of sizes $n\\times n$, $n=2^d$ <font color='red'> into 4 blocks</font> of size $\\frac{n}{2}\\times \\frac{n}{2}$\n",
    "- Then we <font color='red'>calculate multiplications</font> in the described formulas <font color='red'>recursively</font>\n",
    "\n",
    "This leads us again to the **divide and conquer** idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example of Strassen algorithm\n",
    " \n",
    " Let's multiply two 2x2 matrices using Strassen's method:\n",
    " \n",
    "$$A = \\begin{bmatrix} 2 & 3 \\\\ 4 & 1 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 & 7 \\\\ 6 & 8 \\end{bmatrix}$$\n",
    " \n",
    "Calculate the 7 products $f_1$ through $f_7$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "f_1 &= (2 + 1)(5 + 8) = 3 \\cdot 13 = 39 \\\\\n",
    "f_2 &= (4 + 1)(5) = 5 \\cdot 5 = 25 \\\\\n",
    "f_3 &= (2)(7 - 8) = 2 \\cdot (-1) = -2 \\\\\n",
    "f_4 &= (1)(6 - 5) = 1 \\cdot 1 = 1 \\\\\n",
    "f_5 &= (2 + 3)(8) = 5 \\cdot 8 = 40 \\\\\n",
    "f_6 &= (4 - 2)(5 + 7) = 2 \\cdot 12 = 24 \\\\\n",
    "f_7 &= (3 - 1)(6 + 8) = 2 \\cdot 14 = 28\n",
    "\\end{align*}$$\n",
    " \n",
    "Now compute the elements of result matrix $C$:\n",
    " \n",
    "$$\\begin{align*}\n",
    "c_{11} &= f_1 + f_4 - f_5 + f_7 = 39 + 1 - 40 + 28 = 28 \\\\\n",
    "c_{12} &= f_3 + f_5 = -2 + 40 = 38 \\\\\n",
    "c_{21} &= f_2 + f_4 = 25 + 1 = 26 \\\\\n",
    "c_{22} &= f_1 - f_2 + f_3 + f_6 = 39 - 25 - 2 + 24 = 36\n",
    "\\end{align*}$$\n",
    " \n",
    "Therefore:\n",
    " \n",
    "$$C = \\begin{bmatrix} 28 & 38 \\\\ 26 & 36 \\end{bmatrix}$$\n",
    " \n",
    "You can verify this equals the result of standard matrix multiplication!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Complexity of the Strassen algorithm\n",
    "\n",
    "#### Number of multiplications\n",
    "\n",
    "Calculation of number of multiplications is a trivial task. Let us denote by $M(n)$ number of multiplications used to multiply 2 matrices of sizes $n\\times n$ using the divide and conquer concept.\n",
    "Then for na&iuml;ve algorithm we have number of multiplications\n",
    "\n",
    "$$ M_\\text{naive}(n) = 8 M_\\text{naive}\\left(\\frac{n}{2} \\right) = 8^2 M_\\text{naive}\\left(\\frac{n}{4} \\right)\n",
    "= \\dots = 8^{d-1} M(2) = 8^{d} M(1) = 8^{d} = 8^{\\log_2 n} = n^{\\log_2 8} = n^3 $$\n",
    "\n",
    "So, even when using divide and coquer idea we can not be better than $n^3$.\n",
    "\n",
    "Let us calculate number of multiplications for the Strassen algorithm:\n",
    "\n",
    "$$ M_\\text{strassen}(n) = 7 M_\\text{strassen}\\left(\\frac{n}{2} \\right) = 7^2 M_\\text{strassen}\\left(\\frac{n}{4} \\right)\n",
    "= \\dots = 7^{d-1} M(1) = 7^{d} = 7^{\\log_2 n} = n^{\\log_2 7} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Number of additions\n",
    "\n",
    "There is no point to estimate number of addtitions $A(n)$ for naive algorithm, as we already got $n^3$ multiplications.  \n",
    "For the Strassen algorithm we have:\n",
    "\n",
    "$$ A_\\text{strassen}(n) = 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 $$\n",
    "\n",
    "since on the first level we have to add $\\frac{n}{2}\\times \\frac{n}{2}$ matrices 18 times and then go deeper for each of the 7 multiplications. Thus,\n",
    "\n",
    "<font size=2.0>\n",
    "\n",
    "$$ \\begin{split}\n",
    "A_\\text{strassen}(n) =& 7 A_\\text{strassen}\\left( \\frac{n}{2} \\right) + 18 \\left( \\frac{n}{2} \\right)^2 = 7 \\left(7 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 18 \\left( \\frac{n}{4} \\right)^2 \\right) + 18 \\left( \\frac{n}{2} \\right)^2 =\n",
    "7^2 A_\\text{strassen}\\left( \\frac{n}{4} \\right) + 7\\cdot 18 \\left( \\frac{n}{4} \\right)^2 +  18 \\left( \\frac{n}{2} \\right)^2 = \\\\\n",
    "=& \\dots = 18 \\sum_{k=1}^d 7^{k-1} \\left( \\frac{n}{2^k} \\right)^2 = \\frac{18}{4} n^2 \\sum_{k=1}^d \\left(\\frac{7}{4} \\right)^{k-1} = \\frac{18}{4} n^2 \\frac{\\left(\\frac{7}{4} \\right)^d - 1}{\\frac{7}{4} - 1} = 6 n^2 \\left( \\left(\\frac{7}{4} \\right)^d - 1\\right) \\leqslant 6 n^2 \\left(\\frac{7}{4} \\right)^d = 6 n^{\\log_2 7}\n",
    "\\end{split}\n",
    "$$\n",
    "</font>\n",
    "\n",
    "(since $4^d = n^2$ and $7^d = n^{\\log_2 7}$).\n",
    "\n",
    "\n",
    "Asymptotic behavior of $A(n)$ could be also found from the [master theorem](https://en.wikipedia.org/wiki/Master_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Total complexity\n",
    "\n",
    "Total complexity is $M_\\text{strassen}(n) + A_\\text{strassen}(n)=$ <font color='red'>$7 n^{\\log_2 7}$</font>. Strassen algorithm becomes faster\n",
    "when\n",
    "\n",
    "\\begin{align*}\n",
    "2n^3 &> 7 n^{\\log_2 7}, \\\\\n",
    "n &> 667,\n",
    "\\end{align*}\n",
    "\n",
    "so it is not a good idea to get to the bottom level of recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accelerating MM by parallelization\n",
    "\n",
    "Historically, parallel implementations of many optimized **BLAS** libraries rely on so-called **bulk synchronous programming model**:\n",
    "* Static work allocation and data distribution\n",
    "* Alternating parallel and communication regions to satisfy data dependencies:\n",
    "    * Parallel execution\n",
    "    * Data communications\n",
    "    * Parallel execution\n",
    "    * Data communications\n",
    "    * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Task-based programming model\n",
    "\n",
    "* Another parallel programming paradigm, which requires total reimplementation of all known algotihms\n",
    "* Entire algorithm is presented as a Directed Acyclic Graph (DAG) of asynchronously executed tasks\n",
    "* Each node of the DAG is a task, that operates on data:\n",
    "    * Incoming edge: input data\n",
    "    * Outgoing edge: output data (if the task changes data)\n",
    "* Special library (e.g. StarPU, PaRSEC or OpenMP) keeps track of all data and executes tasks with satisfied dependencies\n",
    "* Total hardware utilization is increased due to overlapped tasks executions and data communications. Therefore, wall execution time is reduced in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG of tasks for a single Mixer layer: order of execution is a runtime decision\n",
    "<img width=400% src=\"out6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG of tasks for a single Mixer layer: order of execution is a runtime decision\n",
    "<img width=100% src=\"out6_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# StarPU library: sequential task-based programming model for distributed-memory systems\n",
    "\n",
    "## Typical workflow with the StarPU library requires only master thread to run user code:\n",
    "1. Init StarPU and all other related libraries (e.g., MPI, cuBLAS).\n",
    "2. Register data with StarPU.\n",
    "3. Submit tasks, that operate on registered data, into a pool of tasks. Tasks are inserted asynchronously, i.e., master thread continues sequential flow through the program without waiting for the result.\n",
    "4. Wait for all tasks to complete.\n",
    "5. Unregister data and free memory.\n",
    "6. Deinit StarPU and all related libraries (opposite to the initialization order)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# StarPU library: sequential task-based programming model for distributed-memory systems\n",
    "​\n",
    "​\n",
    "## All the rest is done by StarPU automatically:\n",
    "* Task executing workers for each core/device are created. StarPU supports CPU, CUDA, OpenCL and FPGA. Such a support can be extended via certain driver routines.\n",
    "* Data communicating worker is created in case of MPI environment. StarPU supports HPC-oriented communication library NewMadeleine https://gitlab.inria.fr/pm2/pm2.\n",
    "* Task is executed only when all the required input data are on the executing node.\n",
    "* Performance of each low-level kernel is tracked during runtime, while communication speed between different NUMA nodes of a single computer is probed at initialization time (if needed). It helps StarPU to schedule all tasks to reduce wall execution time.\n",
    "* Certain schedulers support work stealing from neighbour nodes.\n",
    "* Resilient distributed computing through asynchronous checkpointing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Strassen algorithm and tensor rank (advanced topic)\n",
    "\n",
    "- It is not clear how Strassen found these formulas. \n",
    "- However, now we can see that they are not artificial.\n",
    "- There is a general approach based on the so-called tensor decomposition technique. \n",
    "- Here by tensor we imply a multidimensional array - generalization of the matrix concept to many dimensions.\n",
    "\n",
    "Let us enumerate elements in the $2\\times 2$ matrices as follows\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} c_{1} & c_{3} \\\\ c_{2} & c_{4}  \\end{bmatrix} =\n",
    "\\begin{bmatrix} a_{1} & a_{3} \\\\ a_{2} & a_{4}  \\end{bmatrix}\n",
    "\\begin{bmatrix} b_{1} & b_{3} \\\\ b_{2} & b_{4}  \\end{bmatrix}=\n",
    "\\begin{bmatrix} \n",
    "a_{1}b_{1} + a_{3}b_{2} & a_{1}b_{3} + a_{3}b_{4} \\\\ \n",
    "a_{2}b_{1} + a_{4}b_{2} & a_{2}b_{3} + a_{4}b_{4} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This can be written as\n",
    "\n",
    "$$ c_k = \\sum_{i=1}^4 \\sum_{j=1}^4 x_{ijk} a_i b_j, \\quad k=1,2,3,4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$x_{ijk}$ is a 3-dimensional array, that consists of zeros and ones:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "x_{\\ :,\\ :,\\ 1} = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "x_{\\ :,\\ :,\\ 2} = \n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "x_{\\ :,\\ :,\\ 3} = \n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\quad\n",
    "x_{\\ :,\\ :,\\ 4} = \n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{split}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Trilinear decomposition\n",
    "\n",
    "To get Strassen algorithm we should do the following trick –– decompose $x_{ijk}$ in the following way\n",
    "\n",
    "$$ x_{ijk} = \\sum_{\\alpha=1}^r u_{i\\alpha} v_{j\\alpha} w_{k\\alpha}. $$\n",
    "\n",
    "This decomposition is called **trilinear tensor decomposition** and has a meaning of separation of variables: we have a sum of $r$ (called rank) summands with separated $i$, $j$ and $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Strassen via trilinear\n",
    "\n",
    "Now we have\n",
    "\n",
    "$$ c_k = \\sum_{\\alpha=1}^r w_{k\\alpha} \\left(\\sum_{i=1}^4  u_{i\\alpha} a_i \\right) \\left( \\sum_{j=1}^4 v_{j\\alpha} b_j\\right), \\quad k=1,2,3,4. $$\n",
    "\n",
    "Multiplications by $u_{i\\alpha}$ or $v_{j\\alpha}$ or $w_{k\\alpha}$ do not require recursion since $u, v$ and $w$ are known precomputed matrices. Therefore, we have only $r$ multiplications of $\\left(\\sum_{i=1}^4  u_{i\\alpha} a_i \\right)$ $\\left( \\sum_{j=1}^4 v_{j\\alpha} b_j\\right)$ where both factors depend on the input data.\n",
    " \n",
    "As you might guess array $x_{ijk}$ has rank $r=7$, which leads us to $7$ multiplications and to the Strassen algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## AlphaTensor\n",
    "\n",
    "Recent [AlphaTensor](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor) paper has shown how modern deep reinforcement learning can be used to get new decompositions of tensors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualization\n",
    "<img width=100% src=\"sasha-slide-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RL interpretation\n",
    "\n",
    "\n",
    "In reinforcement learning agent learns to make **actions** based on the state and reward.\n",
    "\n",
    "In this case, the state is the tensor.\n",
    "\n",
    "The action is subtraction of a rank-one tensor.\n",
    "\n",
    "If you get non-zero in the end, you get the reward.\n",
    "\n",
    "\n",
    "Then, you do millions of different actions, and reinforce good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selected results\n",
    "\n",
    "- Better ranks for certain matrix sizes\n",
    "\n",
    "- New variants for 4x4 Strassen that work on real hardware faster (but only for this specific hardware!)\n",
    "\n",
    "- Better antisymmetric matrix-by-vector product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of MM part\n",
    "- MM is the core of NLA. You have to think in block terms, if you want high efficiency\n",
    "- This is all about computer memory hierarchy\n",
    "- Concept of block algorithms\n",
    "- (Advanced topic) Strassen and trilinear form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
